# Rock-Paper-Scissors-Image-Classification

 The goal of this project is to build a machine learning model that can accurately classify images of rock, paper, and scissors. Specifically, given an image of a hand displaying one of these gestures, the model should output the corresponding class label. Additionally, it serves as a useful case study for exploring the effectiveness of transfer learning models including both VGG16 and ResNet18 for image classification tasks. The dataset used for this project consists of approximately 2188 images of hands displaying rock, paper, or scissors, with roughly equal numbers of examples for each class. ‘Rock’ (726 images), ‘Paper’ (712 images), and ‘Scissors’ (750 images). Here are few images taken from the dataset:
 
![0ePX1wuCc3et7leL](https://user-images.githubusercontent.com/71690933/230923988-322e6b47-7b4d-453e-91e5-82b526fca37c.png)
![1vvcitV1s17gKdbn](https://user-images.githubusercontent.com/71690933/230924024-d0a2f1f3-3563-474f-8a3a-c3f86315f58d.png)
![4IwJ2iL6pFz5ARWA](https://user-images.githubusercontent.com/71690933/230924056-0433f064-5e32-4290-b1ba-3d5b757b680c.png)
